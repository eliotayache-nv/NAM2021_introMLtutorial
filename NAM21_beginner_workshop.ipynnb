{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NAM21_beginner_workshop.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vMQU7g9b-gds"},"source":["# NAM 2021 workshop: Introduction to Machine Learning\n","\n","By Eliot Ayache & Tanmoy Laskar (University of Bath)\n","\n","Author contact: https://eliotayache.github.io/, https://scholar.harvard.edu/laskar\n","\n","Delivered on 19 July 2021 at the 2021 National Astronomy Meeting\n","\n","*See the end of this document for reuse conditions and copyright information.*"]},{"cell_type":"markdown","metadata":{"id":"QPKGkt405Amx"},"source":["\n","\n","[NAM Abstract](https://nam2021.org/science/parallel-sessions/details/2/77):\n","Recent years have seen a rise of data-driven research in several branches of astronomy and astrophysics as both hardware and software improvements have significantly facilitated the implementation of new statistical data analysis techniques. Multiple out-of-the-box toolkits such as tensorflow and scikit-learn are making machine learning more accessible than ever to non-specialists. However, the sheer volume of literature and overwhelming number of packages available makes it challenging for the self-taught researcher to find the right direction to approach these methods and to efficiently apply them to scientifically relevant questions in their field. The objective of this workshop is to provide beginners in machine learning with hands-on experience in implementing and using statistical-learning methods as well as background knowledge necessary to identify the additional tools they would require to solve specific astrophysical problems. In this two-hour tutorial, participants will implement a full machine-learning method from scratch and apply it to an astrophysical problem, while being given the opportunity to get a grasp of the potential and limitations of these statistical methods."]},{"cell_type":"markdown","metadata":{"id":"bTYOOTTiCZbU"},"source":["## Resources\n","For more information on convolutional neural networks, see: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n","\n","For a related tutorial on Keras, see the BathML Keras workshop: https://github.com/owenjonesuob/keras-workshop \n"]},{"cell_type":"code","metadata":{"id":"fsNIAu3nzf0q"},"source":["# Initial import statements\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pFUnv4TNBcYc"},"source":["Our task is going to be to perform classification using supervised machine learning using an astronomy pseudo-example. We will classify images of galaxies into ellipticals and spirals using a convolutional neural network (CNN) with tensorflow."]},{"cell_type":"markdown","metadata":{"id":"z8bc_Q3CMCKj"},"source":["## Generate synthetic data\n","To keep everything self-contained, we are going to generate our (synthetic) data directly in this tutorial. Let's start with the functions to make ellipticals and spirals."]},{"cell_type":"code","metadata":{"id":"k668jP9L0Iig"},"source":["N_image = 28 # Number of pixels per side in image\n","\n","# The getprimes function is just a coordinate transformation\n","# that makes it easy to calculate brightness as a function of pixel position\n","# for the ellipsoid and spiral functions below\n","def getprimes(theta):\n","  x = np.tile(np.linspace(-1,1,N_image).reshape(1,N_image), (N_image,1))\n","  y = np.tile(-np.linspace(-1,1,N_image).reshape(N_image,1), (1,N_image))\n","  sintheta = np.sin(theta)\n","  costheta = np.cos(theta)\n","  if np.ndim(theta) > 0:\n","    x_prime = np.einsum('ij,k->ijk',x,costheta) - \\\n","              np.einsum('ij,k->ijk',y,sintheta)\n","    y_prime = np.einsum('ij,k->ijk',x,sintheta) + \\\n","              np.einsum('ij,k->ijk',y,costheta)\n","  else:\n","    x_prime = x * costheta - y * sintheta\n","    y_prime = x * sintheta + y * costheta\n","  return x_prime, y_prime\n","\n","def addnoise(image, sigma=0.1):\n","  noise_img = np.random.normal(0, sigma, image.shape)\n","  return image+noise_img\n","\n","def ellipsoid(A,e,theta):\n","    B = A*(1.-e)    \n","    x_prime, y_prime = getprimes(theta)  \n","    r_prime = np.sqrt((x_prime/A)**2 + (y_prime/B)**2)\n","    img = np.exp(-r_prime)\n","    noisy_img = addnoise(img)\n","    return(noisy_img)\n","\n","def spiral(A,e,theta,orient=1):    \n","    A *= 2\n","    B = A*(1.-e)\n","    x_prime, y_prime = getprimes(theta)    \n","    r_prime = np.sqrt((x_prime/A)**2 + (y_prime/B)**2)\n","    theta_prime = np.arctan(x_prime/y_prime)\n","    img = np.maximum(np.exp(-r_prime*r_prime * 4)*(4.+np.cos(3* np.pi * r_prime + orient * 2 * theta_prime + np.pi / 2.)) / 5.,\n","                    np.exp(-r_prime*3))\n","    noisy_img = addnoise(img)\n","    return(noisy_img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTaLW7iqCn2J"},"source":["What do these functions do? Let's have a look at a couple of examples"]},{"cell_type":"code","metadata":{"id":"3u4QRjrj6IHF"},"source":["el = ellipsoid(A=0.5,e=0.3,theta=0.3); plt.imshow(el, cmap='Greys'); plt.show()\n","sp = spiral(A=0.7,e=0.3,theta=0.3); plt.imshow(sp, cmap='Greys'); plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nb7LAC8rDW-p"},"source":["Now let's make a bunch of these for training.\n"]},{"cell_type":"code","metadata":{"id":"bG6dhqQ2zj8i"},"source":["N_els = 5000\n","N_sps = 5000\n","np.random.seed(seed=17)\n","els = ellipsoid(A    = np.random.uniform(low=0.3, high=0.9, size=N_els), \n","                e    = np.random.uniform(low=0.1, high=0.5, size=N_els), \n","                theta= np.random.uniform(low=0.1, high=0.9, size=N_els))\n","sps =    spiral(A    = np.random.uniform(low=0.3, high=0.9, size=N_sps), \n","                e    = np.random.uniform(low=0.1, high=0.5, size=N_sps), \n","                theta= np.random.uniform(low=0.1, high=0.9, size=N_sps),\n","                orient=np.random.randint(2, size=N_sps)*2-1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kf8GWqC3EmVc"},"source":["Let's plot a random subset of these images to see what we've generated"]},{"cell_type":"code","metadata":{"id":"zgv-eT6KBFvl"},"source":["# Function to plot a random subset \n","def plotrandomsubset(imagelist, n):  \n","  randx = np.random.randint(0,len(imagelist)-1,n*n)\n","  from mpl_toolkits.axes_grid1 import ImageGrid\n","  \n","  fig = plt.figure(figsize=(2*n, 2*n))\n","  grid = ImageGrid(fig, 111,  # similar to subplot(111)\n","                  nrows_ncols=(n, n),  # creates nxn grid of axes\n","                  axes_pad=0.0,  # pad between axes in inch.\n","                  )\n","  for ax, im in zip(grid, [imagelist[:,:,i] for i in randx]):\n","      # Iterating over the grid returns the Axes.\n","      ax.imshow(im, cmap='Greys')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q-3uFQ1zE5at"},"source":["We'll plot a 4x4 grid of images from each class"]},{"cell_type":"code","metadata":{"id":"Tv41DG3GE0Ut"},"source":["print(\"Subset of elliptical class:\")\n","plotrandomsubset(els, 4)\n","\n","print(\"Subset of spiral class:\")\n","plotrandomsubset(sps, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"chhFTIgDGvUg"},"source":["Now things start getting interesting. Let's set up the labels for our input data. We will start with zeros for ellipticals and 1 for spirals. This will later be coded into numpy arrays using one-hot encoding (depending on the kind of encoding you use for the class labels, you will need to pick a compatible kind of cross-entropy during in the model training process, but we'll get to that later). "]},{"cell_type":"code","metadata":{"id":"5SRYjTKo5iNr"},"source":["# 0 = elliptical, 1 = spiral\n","input_labels = np.array([[0]*N_els+[1]*N_sps])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txPqZ00_HiYG"},"source":["This is a binary classification problem (0 or 1), and it is possible to proceed with these labels. But for flexibility, we are now going to transform the class labels from binary to categorical (one-hot-encoded) labels. This will be useful if we later on want to expand our model to try and classify data from more than two categories.\n"]},{"cell_type":"code","metadata":{"id":"FUFPQ2vbHeQ_"},"source":["from tensorflow.keras.utils import to_categorical\n","labels_grouped = to_categorical(input_labels)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4zW3ANO_JAzz"},"source":["What did that do? Let's take a look at the shape of the labels."]},{"cell_type":"code","metadata":{"id":"LigwSgZEIudr"},"source":["print(labels_grouped.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kNJKWtorJPvK"},"source":["The \"grouped\" reminds us that these labels are still grouped by N_els ellipticals followed by N_sps spirals:"]},{"cell_type":"code","metadata":{"id":"D3dq9CHDIuem"},"source":["print(labels_grouped[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPCPj_UqI4ay"},"source":["print(labels_grouped[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L9G0hMx_JW3J"},"source":["We'll shuffle it up in a second.\n","\n","But first, we also need to concatenate the actual images of N_els ellipticals and N_sps spirals together."]},{"cell_type":"code","metadata":{"id":"gZ7M1duFIiTc"},"source":["data_grouped  = np.append(els, sps, axis=2)\n","data_grouped.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IoPX3-IQKDnK"},"source":["OK, that has stacked all of the images together into a giant N_image x N_image x N_els + N_sps cube. It turns out that most ML algorithms require the sample number to be the *first* axis. So we need to move the third axis of this object to be the first. \n","\n","**Warning**\n","\n","If you miss this step, it *will* cause trouble later!"]},{"cell_type":"code","metadata":{"id":"KfqN_zl1JkRy"},"source":["data_grouped  = np.moveaxis(np.append(els, sps, axis=2), 2, 0)\n","print(data_grouped.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7V4pW8gFK2rn"},"source":["The order of the remaining axes is preserved by np.moveaxis(). \n","\n","Now we are going to shuffle the data and label rows so that they are not in any particular order. There are automatic tools for doing this, but we'll do it manually here because we want to make sure that the labels are shuffled in exactly the same way as the images."]},{"cell_type":"code","metadata":{"id":"AVEZuLFyCAu1"},"source":["m = data_grouped.shape[0]\n","shuffle_ix = np.arange(m)\n","np.random.shuffle(shuffle_ix)   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ip11INZ8IYrd"},"source":["print(shuffle_ix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SveSx7yxIVVz"},"source":["X = data_grouped[shuffle_ix]\n","y = labels_grouped[shuffle_ix]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s8wxjP9ILnZR"},"source":["To check what that did, let's plot a random subset from this data. We need to move the first axis to the end in order for the plotting function we wrote earlier to work properly."]},{"cell_type":"code","metadata":{"id":"hQKJOWGxLjoe"},"source":["print(\"Subset of data\")\n","plotrandomsubset(np.moveaxis(X,0,2), 4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MnS6ReZXkw-p"},"source":["## Data Normalization and Standardization\n","\n","At this point, it is a good idea to make sure the values of the features (aka the pixel values, whatever will go on to become inputs to the neural network) are small numbers with absolute magnitude < 1. This is to prevent the need for very large or very small weights during the training process, because remember, the weights are multiplied by the input values and summed up ... so we need nicely behaved input values that are not too large or too small.\n","\n","In any other ML application, now is when you might want to consider normalizing (set the max absolute magnitude to 1) and/or standardizing the data (ensure the distribution of input values roughly follows a unit normal distribution). \n","\n","Our pixel values are all pretty small and well distributed, so we aren't going to worry about this."]},{"cell_type":"code","metadata":{"id":"WVS-dWjNl316"},"source":["plt.hist(X.reshape(N_image*N_image*10000),bins=50); plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fC07ds0XL6il"},"source":["## Generate training and testing data\n","There are multiple ways of doing this - we could take the simple first step\n","of splitting the data into an 80% training sample and a 20% testing sample. "]},{"cell_type":"code","metadata":{"id":"neF3f1QEh5HD"},"source":["from sklearn.model_selection import train_test_split\n","X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3VuD4lS-ilBu"},"source":["print(X_train_full.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uNK1FLjgilAs"},"source":["print(y_train_full.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GdJ1yuDfik_t"},"source":["print(X_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hsiNK5cNMrKo"},"source":["Another way of doing this is to load the data into a tf.data.Dataset object (i.e. turn the numpy arrays into tensorflow Tensors) and then use keras preprocessing tools to do this split in one line. This is especially helpful if you want to use tf operations later, see https://www.tensorflow.org/tutorials/load_data/numpy\n","\n","We will not attempt this here, but here's the code to do that if you want to try it later. "]},{"cell_type":"code","metadata":{"id":"9RstuAdUMhRR"},"source":["# import tensorflow as tf\n","# test_dataset  = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n","# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","\n","# But then we also need to batch the datasets, otherwise training will not work.\n","# We can optionally shuffle the data now, though in our case\n","# we have already done that manually earlier\n","\n","# BATCH_SIZE = 64\n","# SHUFFLE_BUFFER_SIZE = 100\n","# train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n","# test_dataset = test_dataset.batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SDVL_TRGiYkK"},"source":["We will not perform cross-validation in this tutorial (where the test/train split is randomly shuffled and the model is refit several times, in order to reduce bias at the cost of introducing additional model variance). But we *will* further split the training data into 75% training and 25% validation. This validation set will become useful for hyperparameter tuning later on in this tutorial. "]},{"cell_type":"code","metadata":{"id":"kOTJWlauEdCF"},"source":["X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYCsXkHsRfp5"},"source":["print(X_train.shape)\n","print(X_val.shape)\n","print(X_test.shape)\n","print(y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mw8wuK6_V7C1"},"source":["## Build and train CNN model\n"]},{"cell_type":"markdown","metadata":{"id":"DEm_AdAAV_kG"},"source":["*New vocabulary: Neural network, dense network, neuron, weights, bias, activation*\n","\n","Before we dive into *convolutional* neural nets, it might be a good idea to remind ourselves of the basic principles of neural networks. After we go through this theory, we will design and train a neural network.\n","\n","Conceptually, neural networks consist of nodes (neurons) that spit out a weighted sum of a section of the input. Fundamentally, these are simply a series of matrix operations (hence \"tensor\"flow).\n","\n","Here is the simplest form of a neural network, a *a fully connected* (aka *densely connected*, or simply, *dense*) neural network:\n","\n","<img src=\"https://otexts.com/fpp2/nnet2.png\" alt=\"Fully connected Neural Network\" width=\"400\"/>\n","\n","All inputs are connected to each neuron in the hidden layer. The output* of each blue neuron can be written as \n","\n","$z_j = b_j + \\sum_{i=1}^{N_{\\rm input}} w_{ij} x_i$, \n","\n","where $x_i$ are the inputs, $w_{ij}$ are the weights of the $j^{\\rm th}$ neuron, and $b_j$ are the *biases*. These weights and biases are the free parameters of the network. They are initialized to random values when we first create the network. The process of *training* the network involves adjusting these free parameters until the result matches the expected output (e.g. class labels). \n","\n","\\*Technically, this is the output before the activation function is applied.\n","\n","Each neuron acts on *all* input elements: \n","\n","<img src=\"https://miro.medium.com/max/500/1*Kg5cA0WNLjDnS3F6gbwFYQ.gif\" alt=\"Fully connected Neural Network\" width=\"400\"/>\n","\n","The final step is to introduce an *activation*. Above, we have assumed that the activation of each neuron is linear, i.e., the neuron's output is a simple linear combination of the inputs. Mathematically, we could write, the neuron's final output as,\n","\n","$\\phi_j(z_j) = z_j$\n","\n","A neural network with only linear activation functions essentially reduces to a linear regression problem... and there are well-known ways of solving those problems (involving matrix inversion) that do not require the full machinery of neural networks. Introducing non-linearity into the neuron's activation is what gives neural networks their rich, complex behaviour. **We do this by using non-linear activation functions.**\n","\n","Some common examples of non-linear activations are \n","\n","<img src=\"https://miro.medium.com/max/1800/1*EmTYifwsrA6YNPI2vYRf7g.gif\" alt=\"Fully connected Neural Network\" width=\"600\"/> \n","\n","Note ELU = Exponential Linear Unit, ReLU = Rectified Linear Unit.\n","\n","Now that we've talked about the basic principles of a neural network, we will move on to *convolutional* neural networks. "]},{"cell_type":"markdown","metadata":{"id":"MO7KhA1j03oz"},"source":["### Convolutional neural networks (CNNs)\n","\n","*New vocabulary: CNN, filter, stride, channel, slice, padding*\n","\n","A *convolutional* neural network additionally performs a convolution on the inputs using a set of *filters*. Here is an example of a 3x3 filter operating on a 5x5 input grid (in our case, this will be a 28x28 image).\n","\n","<img src=\"https://miro.medium.com/max/2000/1*YvlCSNzDEBGEWkZWNffPvw.gif\" alt=\"Fully connected Neural Network\" width=\"400\"/>\n","\n","Q1: How many weights does each 3x3 filter have? \n","(a) 1 \n","(b) 3 \n","(c) 9 \n","(d) 28x28x9=7056\n","\n","Q2: How many biases does each 3x3 filter have? \n","(a) 1 \n","(b) 3 \n","(c) 9 \n","(d) 7056 \n","\n","Q3: How many free parameters are associated with each 3x3 filter?\n","\n","#### Padding\n","\n","If we don't want the input dimensions to change, we can *pad* the convolution. Here's what happens when we pad by 1.\n","\n","<img src=\"https://miro.medium.com/max/2000/1*gXAcHnbTxmPb8KjSryki-g.gif\" alt=\"Fully connected Neural Network\" width=\"400\"/>\n","\n","We will use the *padding=\"same\"* option in our convolutional layer to calculate the amount of padding for us automatically. \n","\n","Note that in this way of convolving the same input pixel contributes to multiple output pixels. We can make the outputs more independent using a *stride* greater than 1. \n","\n","<img src=\"https://miro.medium.com/max/2000/1*34_365CJB5seboQDUrbI5A.gif\" alt=\"Fully connected Neural Network\" width=\"400\"/>\n","\n","<img src=\"https://miro.medium.com/max/2000/1*WpOcRWlofm0Z0EDUTKefzg.gif\" alt=\"Fully connected Neural Network\" width=\"400\"/>\n","\n","(Gifs source: [Aqeel Anwar, What is a CNN?](https://towardsdatascience.com/a-visualization-of-the-basic-elements-of-a-convolutional-neural-network-75fea30cd78d))"]},{"cell_type":"markdown","metadata":{"id":"kyILv8VKxN72"},"source":["With that in mind, it is now time to build our own neural network, huzzah! We'll start with some imports."]},{"cell_type":"code","metadata":{"id":"LMWTwMQh8LON"},"source":["import tensorflow as tf\n","from tensorflow.keras import datasets, layers, models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1DDRZIuLWTWv"},"source":["What is *Keras*? from keras.io : \n","> Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow.\n","\n","*Keras* is a programming interface that abstracts away and packages up the complexity of networks into objects that represent data and results and functions that represent actions, which makes coding up ML applications extremely easy.\n","\n","We are now going to define our network architecture using Keras *layers* objects. Here's where some of the art of machine learning comes in (though, as we'll see later, even this step can be optimized!). \n","\n","There are multiple ways of doing this. The simpler technique is the *sequential* model, which is what we are going to use here. For more complex models, you can use the full power of the [Keras *functional* API](https://keras.io/guides/functional_api/). \n","\n","Let's build a very simple model with one hidden layer.\n"]},{"cell_type":"code","metadata":{"id":"uE9Ehh6i1QLd"},"source":["fs = 3 # filter_size\n","\n","# Define layers (named, so we can nab them later)\n","inputs = layers.Input(shape=(N_image, N_image, 1))\n","conv1A = layers.Conv2D(16, (fs, fs), activation='relu', padding=\"same\",strides=(1,1))(inputs)\n","model = models.Model(inputs, conv1A)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZRI8j431r5G"},"source":["Q4: How many free parameters have we introduced to the model at this point (remember to count the bias!)\n","\n","Let's see what our model looks like so far:"]},{"cell_type":"code","metadata":{"id":"fF71Qfbq1mhy"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8VN4Y7RF77Hb"},"source":["Cool, let's write out the rest of the model. This will include a series of convolution layers, pooling layers, and flatten step, and finally, a series of densely connected layers."]},{"cell_type":"code","metadata":{"id":"hjndCCLXWBxh"},"source":["fs = 3 # filter_size\n","\n","inputs = layers.Input(shape=(N_image, N_image, 1))\n","conv1A = layers.Conv2D(16, (fs, fs), activation='relu', padding=\"same\",strides=(1,1))(inputs)\n","conv1B = layers.Conv2D(16, (fs, fs), activation='relu', padding=\"same\",strides=(1,1))(conv1A)\n","pool1 = layers.MaxPooling2D((2, 2))(conv1B)\n","conv2A = layers.Conv2D(32, (fs, fs), activation='relu', padding=\"same\")(pool1)\n","conv2B = layers.Conv2D(32, (fs, fs), activation='relu', padding=\"same\")(conv2A)\n","pool2 = layers.MaxPooling2D((2, 2))(conv2B)\n","conv3A = layers.Conv2D(64, (fs, fs), activation='relu', padding=\"same\")(pool2)\n","conv3B = layers.Conv2D(64, (fs, fs), activation='relu', padding=\"same\")(conv3A)\n","flatten1 = layers.Flatten()(conv3B) \n","dense1 = layers.Dense(64,activation='relu')(flatten1)\n","dense2 = layers.Dense(32,activation='relu')(dense1)\n","dense3 = layers.Dense(2,activation='softmax')(dense2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8dPyXifB439i"},"source":["We have used the *softmax* activation to turn our outputs into probabilities that will sum to unity. The softmax function is defined as \n","$\\sigma (\\vec{z})_i = \\frac{e^{z_i}}{\\Sigma_1^{K}{e^{z_i}}}$, for $i$ = 1, ..., $K$.\n","\n","There's more about softmax functions here:\n","https://machinelearningmastery.com/softmax-activation-function-with-python/\n","and https://en.wikipedia.org/wiki/Softmax_function\n","\n","Right, let's connect up our model and see what it looks like!"]},{"cell_type":"code","metadata":{"id":"Lv7qUZU56AHv"},"source":["model = models.Model(inputs, dense3)\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HPHUT83z82-B"},"source":["Q5: Can you make sense of the number of parameters for each of the dense layers? Hint: all outputs from the previous layer are now connected as inputs to each dense layer. "]},{"cell_type":"code","metadata":{"id":"Wnt5bn8l80Dh"},"source":["print(3136*64+64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-O7mfRMWR2fv"},"source":["#### Training\n","\n","Before we can use our model, we must compile it. This is also where we define our loss function (what exactly we are trying to optimize)."]},{"cell_type":"code","metadata":{"id":"2pG4uaYKIITC"},"source":["model.compile(optimizer='SGD',\n","              metrics=['accuracy'],\n","              loss='categorical_crossentropy')\n","            # learning_rate=...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80yF0AL3Pyzw"},"source":["---\n","Alright, let's take take a quick break from the coding here and explain what's happening. Compiling the model means specifying the various aspects of the training process.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RmcOdS2o9ePW"},"source":["#####The training process - Backpropagation\n","\n","Training is done using the **backpropagation of error** algorithm. Intuitively, we measure the difference between the output and a target output, the error, and we update the network parameters (the weights) to match the output to the target. This is done with **gradient descent** in which we follow the direction of steepest decrease of the error as a function of the weights. For this we need to compute the partial derivatives with respect to the weights. Each partial derivative depends on the absolute error and the derivatives in the layers placed after it. \n","\n","For those of us that need a bit more to be convinced, here is the mathematical expression for the partial derivatives:\n","\n","(The derivation is available on https://en.wikipedia.org/wiki/Backpropagation)\n","\n","$ \\frac{\\partial E}{\\partial w_{ij}} = o_i \\delta_j $\n","\n","with\n"]},{"cell_type":"markdown","metadata":{"id":"zHS7T-mDFiFl"},"source":["\n","![backprop.png](data:image/png;base64,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)\n"]},{"cell_type":"markdown","metadata":{"id":"YOXsZfxKFiHm"},"source":["Here $E$ is the error, $w_{ij}$ the weights, $z_j$ the weighted sum computed by each neuron before activation, $\\phi$ the activation function applied on this sum, and $L$ all the neurons directly conneted to the ouput $o_j$ of neuron $j$. The recursive character of the gradient calculation is clearly visible in the sum on the elements of $L$ in which we need $\\delta_l$.\n","\n","\n","And so we can update the weights using a pre-specified learning rate $\\eta$:\n","\n","$ \\Delta w_{ij} = -\\eta \\frac{\\partial E}{\\partial w_{ij}} $.\n","\n","Hence, for each training step, we need to: \n","- first compute the error **(forward pass)**,\n","- propagate the error from the last layer to the first **(backwards pass - or backpropagation)**."]},{"cell_type":"markdown","metadata":{"id":"x4Tc2UrWOYsu"},"source":["##### Loss function\n","Of course, we are still missing an error (or loss) function for the output layer. \n","\n","There is a whole zoo of loss functions that can be used. Traditionally, the *mean squared error* is used on *regression* problems. Here we are doing **binary classification**. However we can apply the same methods as in the more general **multi-class** problem which relies on one-hot encoding (which is the way we prepared our dataset).\n","\n","We can use the **categorical-cross entropy** which measures the difference between the output discrete probability distribution (obtained with our softmax activation) and our target distribution (here a one-hot encoding).\n","\n","(For more info there is a great introduction to cross-entropy from Jason Brownlee https://machinelearningmastery.com/cross-entropy-for-machine-learning/)"]},{"cell_type":"markdown","metadata":{"id":"eF67Eur6SLmj"},"source":["#####Optimizer\n","\n","Finally, the simple gradient descent we have described can be significantly improved and several optimizers are implemented in Keras, which offer better results depending on the problem: Stochastic gradient Descent (SGD; which we use in this tutorial, as it is the most basic optimizer), ADAM, AdaDelta, RMSProp... \n","\n","<img src=\"https://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif\" alt=\"Comparison of optimizer performance\" width=\"300\"/>\n","(Image credit Sebastian Ruder)\n","\n","(For more info see this excellent article by Sebastian Ruder: https://ruder.io/optimizing-gradient-descent/)"]},{"cell_type":"markdown","metadata":{"id":"UpHraDNqTt3i"},"source":["\n","\n","---\n","\n","\n","Okay, let's fit the model! That's easy to do with a model.fit() command. \n","\n","\n","\n","The first argument to model.fit() is the \"x\" or \"independent variable\", which in our case is a set of images. The second argument is the \"y\" or \"dependent variable\", or \"target classification\", which in our case is an array of one-hot encoded labels.\n"]},{"cell_type":"code","metadata":{"id":"bGEmokb2MQh8"},"source":["history = model.fit(X_train, y_train, epochs=30, batch_size=100, validation_data=(X_val, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9bJkDdx2h0F"},"source":["A couple of things to explain here: \n","\n","- We store in `history` metrics for each epoch of the training. They will be usefull to assess convergence.\n","- The `batch_size` is the size of the sample we feed to the network for each weight update. It can be at most the sample size, but a large batch size leads to **overfitting**. Decreasing the batch size means that the model has less information to carry out this update, leading to less overfitting. We feed successive batches eventually covering the whole sample after one **epoch**.\n","- `epochs` is the number of epochs to train for."]},{"cell_type":"markdown","metadata":{"id":"IgOjVS20T3sJ"},"source":["We can also directly fit using the tf.data.Dataset objects we created earlier.\n","If you want to try this, be sure to re-run the model definition and compilation steps first! Otherwise you will be using the pre-trained model from the previous step. Then, comment out the first command in this cell, and uncomment this next one."]},{"cell_type":"code","metadata":{"id":"WxRih56zT36N"},"source":["# model.fit(train_dataset, epochs=30)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Zj2FLOmT_gR"},"source":["Note that when the data have already been packaged into tf.data.Dataset objects,we no longer need to provide data & labels to model.fit() separately."]},{"cell_type":"markdown","metadata":{"id":"gwh7cX3LUcZg"},"source":["## Validation\n","\n","Great, we can do very well on the training set! But how well does the model work on data it has never seen before? For that, let's generate predictions on the test data set. Remember, the model has not seen these data yet."]},{"cell_type":"code","metadata":{"id":"Yo1CYc0RHu6g"},"source":["predictions = model.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tQgFiLh7VHaS"},"source":["This tells us a \"probability\" that the object is in class 0 or class 1. We can now convert each of these to a class label, running from 0 to N, where N is the number of classes\n"]},{"cell_type":"code","metadata":{"id":"Pvfopa6lVCyb"},"source":["predictions_labels = tf.argmax(predictions, axis = 1)\n","test_labels        = tf.argmax(y_test, axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrlPe3AXVc89"},"source":["We can use these class labels to calculate a confusion matrix"]},{"cell_type":"code","metadata":{"id":"z8pTtFptVCzj"},"source":["from sklearn.metrics import confusion_matrix\n","print(confusion_matrix(test_labels,predictions_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B26E47arViZ_"},"source":["There's also a tensorflow version of this function"]},{"cell_type":"code","metadata":{"id":"zDPDrHzCVilj"},"source":["# tf.math.confusion_matrix(test_labels,predictions_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jSG7JXkGVnKL"},"source":["This tells us how many objects of class 0 are classified as 0 or 1 and how many objects of class 1 are classified as class 0 or 1. You can get fancy and turn this into fractions and then plot it to make the shiny confusion matrix plots people have in their papers. As far as we know, you need to write your own code to make such a plot."]},{"cell_type":"code","metadata":{"id":"QFxxOoRe3vlt"},"source":["# sklearn also has a shiny classification report function, which will \n","# calculate the precision, recall, and f1-scores for us\n","\n","from sklearn.metrics import classification_report\n","print(classification_report(test_labels,predictions_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36C4-aw_MZMm"},"source":["- The precision (or correctness) `true_positive/(true_positive+false_positive)` is the ability of the classifier not to label as positive a sample that is negative.\n","- The recall (or completeness) `true_positive/(true_positive+false_negative)` is the ability of the classifier to find all the positive samples.\n","- The F1 score is the harmonic mean of the two and measures the preformance of the classifier : $F_1 = \\frac{2}{\\mathrm{precision}^{-1}+\\mathrm{recall}^{-1}}$"]},{"cell_type":"markdown","metadata":{"id":"oazxnkZrOTr8"},"source":["Now let us see how fast the model has converged. For this we plot the metrics as function of the epochs. This measure of convergence will be used under the hood when tuning the hyperparameters of the model."]},{"cell_type":"code","metadata":{"id":"QllLcmUyBbKX"},"source":["# Let us see how fast the model has converged\n","# For this we can plot the training and validation accuracy (or the loss)\n","# as a function of epoch number\n","\n","def convergence(history):\n","\n","    history = history.history\n","\n","    loss = history[\"loss\"]\n","    val_loss = history[\"val_loss\"]\n","    nepochs = len(loss)\n","\n","    accuracy = history[\"accuracy\"]\n","    val_accuracy = history[\"val_accuracy\"]\n","\n","    plt.plot(np.arange(nepochs), loss, \"k-\", label=\"training loss\")\n","    plt.plot(np.arange(nepochs), val_loss, \"k--\", label=\"validation loss\")\n","    plt.ylabel(\"loss\")\n","    plt.xlabel(\"epoch\")\n","    plt.legend(frameon=False)\n","    plt.show()\n","\n","    plt.plot(np.arange(nepochs), accuracy, \"k-\", label=\"training accuracy\")\n","    plt.plot(np.arange(nepochs), val_accuracy, \"k--\", label=\"validation accuracy\")\n","    plt.ylabel(\"accuracy\")\n","    plt.xlabel(\"epoch\")\n","    plt.legend(frameon=False)\n","    plt.show()\n","\n","convergence(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OK89MvbIKgxc"},"source":["## Hyperparameter tuning\n","\n","The model can further be improved by optimizing various parameters.\n","- Optimisation of network architecture (number/size of layers...)\n","- Optimisation of training parameters (learning rate, number of epochs...)\n","\n","Several modules exist to train Keras models: Hyperas, Talos, Keras Tuner...\n","\\\n","Hyperas does not support Tensorflow 2.0 and Talos does not have Bayesian Optimisation. \n","\n","**Keras Tuner** is the most complete package currently available\n","and can be used on Sequential and Functional models.\n","\n","(Before we start, this post by Ke Gui might save beginners  a lot of time when debugging: https://kegui.medium.com/a-few-pitfalls-for-kerastuner-beginner-users-13116759435b)\n","\n","Okay let's get to it!"]},{"cell_type":"code","metadata":{"id":"DLFhGWswKfEq"},"source":["#installing and importing Keras Tuner\n","!pip install -q -U keras-tuner\n","import kerastuner as kt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CEMWo6GgNKhq"},"source":["The model architecture, including the parameters to optimise, are contained in a model builder function that takes as argument a hyperparameter `hp` attribute of the HyperModel class. This attribute can be used in the following ways:\n","- Parameter tuning: Instead of passing constants to layer keyword arguments, we pass a set of hyperparameters\n","- Model Architecture: can be changed functionally based on a set of hyperparameters."]},{"cell_type":"markdown","metadata":{"id":"HMowMEEiPFL-"},"source":["Here we optimise for:\n","   - Number of convolutional blocks (hyperparameter `\"conv_blocks\"`)\n","   - Number of filters in each convolutional block (`\"filters_i\"`)\n","   - Number of units in the first and second dense layers (`\"units_01\"` and `\"units_02\"`)\n","   - Learning rate (`\"learning_rate\"`)\n","\n","We use the functions `hp.Int()` and `hp.Choice()` to specify a set of integers, or a set of user-defined values, respectively, for a hyperparameter.\n"]},{"cell_type":"code","metadata":{"id":"9iZ_HGiaLKMW"},"source":["# Example inspired from the Keras Team tutorial:\n","# https://github.com/keras-team/keras-tuner/blob/master/examples/cifar10.py\n","\n","def model_builder(hp):\n","\n","  filter_size = 3\n","  fs = filter_size\n","  \n","  from tensorflow.keras.layers import Input\n","  # Beginning of our model\n","  # ----------------------\n","  inputs = Input(shape=(N_image, N_image, 1)) # input layer\n","  x = inputs  # giving a generic name to the tensor going through the model\n","  # We can re-use the same name for multiple layers, making it easier to\n","  # amend the functional model.\n","\n","  # Choosing the number of convolutional blocks\n","  for i in range(hp.Int(\"conv_blocks\", 2, 4, default=3)):\n","    # choosing the number of filters\n","    filters = hp.Int(\"filters_\" + str(i), 8, 32, step=8) \n","    for _ in range(2): # 2 layers per conv block\n","      x = layers.Conv2D(\n","          filters, kernel_size=(fs, fs), padding=\"same\", activation='relu'\n","          )(x)\n","    x = layers.MaxPooling2D((2, 2))(x)\n","\n","  x = layers.Flatten()(x)\n","\n","  # optimising the nuber of units in the first dense layer\n","  hp_units_01 = hp.Int('units_01', min_value=32, max_value=128, step=32)\n","  x = layers.Dense(units=hp_units_01, activation='relu')(x)\n","\n","  # optimising the nuber of units in the second dense layer\n","  # (notice that both hyperparameter objects are name differently)\n","  hp_units_02 = hp.Int('units_02', min_value=16, max_value=64, step=16)\n","  x = layers.Dense(units=hp_units_02,activation='relu')(x)\n","\n","  outputs = layers.Dense(2,activation='softmax')(x)\n","  # ---------------\n","  # End of our model\n","\n","  # Set up the model using the functional API\n","  model = models.Model(inputs, outputs)\n","\n","  # Tune the learning rate for the optimizer\n","  # Choose an optimal value from 0.01, 0.001, or 0.0001\n","  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","\n","  # copile the model\n","  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n","                loss=tf.keras.losses.CategoricalCrossentropy(),\n","                metrics=['accuracy'])\n","  \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zz4ZNskedjm2"},"source":["Now that we have declared our model builder, we need to initialise the tuner which will operate the search in the hyperparameters.\n","Keras Tuner has four tuners available - *RandomSearch*, *Hyperband*, *BayesianOptimization*, and *Sklearn*\n","\n","We will use the state of the art **Hyperband** algorithm which presents the advantage of providing early-stopping for poor models and parallelisation. However the process remains the same for all other optimisers."]},{"cell_type":"code","metadata":{"id":"gW5nnZZJLLEd"},"source":["tuner = kt.Hyperband(model_builder,             # hypermodel\n","                     objective='val_accuracy',  # score\n","                     max_epochs=20)\n","\n","# create a callback for early stopping\n","# stops if validation loss does not increase for more than 'patience' epochs\n","stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QE1EA3mOLQIH"},"source":["# Searching for the best parameters\n","tuner.search(X_train_full, y_train_full,\n","             epochs=20, validation_split=0.2, callbacks=[stop_early])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mX7mfIr9ZT0t"},"source":["best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n","  # returns a list of num_trials hyperparameters ranked from best to worst\n","\n","# printing information about best model:\n","print(f\"\"\"\n","Best Model:\n","conv_blocks: {best_hps.get('conv_blocks')}\n","filters_0 = {best_hps.get('filters_0')}\n","filters_1 = {best_hps.get('filters_1')}\n","filters_2 = {best_hps.get('filters_2')}\n","units_01 = {best_hps.get('units_01')}\n","units_02 = {best_hps.get('units_02')}\n","learning_rate = {best_hps.get('learning_rate')}\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dky0uMggaIQ6"},"source":["Keras Tuner can be used to optimise more parameters such as the **activation function** of a given layer, the **gradient descent** algorithm to use etc...\n","\n","We can now train our best model before evaluating its performance."]},{"cell_type":"code","metadata":{"id":"3ib0t2JPUjoE"},"source":["# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n","model = tuner.hypermodel.build(best_hps)\n","history = model.fit(X_train, y_train, epochs=20, validation_split=0.2)\n","\n","val_acc_per_epoch = history.history['val_accuracy']\n","best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n","print('Best epoch: %d' % (best_epoch,))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eHX99EVsXEGb"},"source":["hypermodel = tuner.hypermodel.build(best_hps)\n","\n","# Retrain the model\n","hyperhistory = hypermodel.fit(X_train, y_train, epochs=best_epoch, validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TDYL5vaPb0aH"},"source":["Let's evaluate our new model:"]},{"cell_type":"code","metadata":{"id":"_18qVhdCXlak"},"source":["convergence(hyperhistory)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJaXmiRqbXwN"},"source":["hyperpredictions = hypermodel.predict(X_test)\n","hyperpredictions_labels = tf.argmax(hyperpredictions, axis = 1)\n","test_labels        = tf.argmax(y_test, axis = 1)\n","print(confusion_matrix(test_labels,hyperpredictions_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KvGBzAtedU6X"},"source":["Our original model was already showing really good performance, and our classification problem is quite a simple one, so the gain we have here is marginal. However, when working on more complex problems, significant improvement can be expected from tuning our model properly."]},{"cell_type":"markdown","metadata":{"id":"O9UbxOUOjfRq"},"source":["---\n","---\n","# Final thoughts\n","\n","Thanks for following this tutorial! We've covered the following:\n","- How to prepare your data for image classification.\n","- How to build and train a CNN using tensorflow and Keras.\n","- How to evaluate the performance of a classification model.\n","- How to optimise the model for faster training and better classification.\n","\n","We've also introduced the following libraries for ML: **Tensorflow, Keras, Scikit-Learn**.\n","\n","In a lot of applications where datasets have various features (unlike images with only pixels), **Pandas** will become very helpful. Finally, in these cases, **standardisation** of your dataset will be crucial if you want your model to converge, which we have not covered in this tutorial.\n","\n","That's it!\n","\n","Happy Machine Learning!"]},{"cell_type":"markdown","metadata":{"id":"TnX7BMD1AI_g"},"source":["## Reusing or adapting this tutorial\n","If you find this tutorial helpful and would like us to deliver it at your institution, please contact the authors. \n","\n","We hope this tutorial gives you a quick, practical introduction to the world of machine learning for astronomy. Academic reward systems for service work like this are limited. If the use of this tutorial enables you to go on to publish any research articles in the future, please consider listing us in your acknowledgements.  \n","\n","This work is released under the [Creative Commons CC BY-NC-SA license](https://creativecommons.org/licenses/by-nc-sa/4.0/). You are free to distribute, remix, adapt, and build upon this material in any medium for non-commercial purposes, provided you attribute the original source and the creators. Any adapted material based on this tutorial must be released under identical terms. \n","\n","<img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Fully connected Neural Network\" width=\"200\"/>\n","\n","\n"]}]}
